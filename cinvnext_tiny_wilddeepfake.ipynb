{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install webdataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:55:01.045136Z","iopub.execute_input":"2025-08-13T16:55:01.045414Z","iopub.status.idle":"2025-08-13T16:55:04.191765Z","shell.execute_reply.started":"2025-08-13T16:55:01.045393Z","shell.execute_reply":"2025-08-13T16:55:04.190566Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: webdataset in /usr/local/lib/python3.11/dist-packages (1.0.2)\nRequirement already satisfied: braceexpand in /usr/local/lib/python3.11/dist-packages (from webdataset) (0.1.7)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from webdataset) (1.26.4)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from webdataset) (6.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->webdataset) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->webdataset) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->webdataset) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->webdataset) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->webdataset) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->webdataset) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->webdataset) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->webdataset) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->webdataset) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->webdataset) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->webdataset) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import kagglehub\nimport os\nimport random\nimport io\nimport copy\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision import datasets, models\nfrom torchvision.models import ConvNeXt_Tiny_Weights\nfrom torch.utils.data import DataLoader, random_split, Subset\nfrom sklearn.metrics import accuracy_score\nfrom PIL import Image\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom torch.utils.data import IterableDataset, ConcatDataset\nimport random\nfrom huggingface_hub import get_token, hf_hub_url, HfFileSystem\nimport webdataset as wds\nfrom collections import defaultdict\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:55:04.193653Z","iopub.execute_input":"2025-08-13T16:55:04.194000Z","iopub.status.idle":"2025-08-13T16:55:08.793362Z","shell.execute_reply.started":"2025-08-13T16:55:04.193970Z","shell.execute_reply":"2025-08-13T16:55:08.792540Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Use GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:55:08.794239Z","iopub.execute_input":"2025-08-13T16:55:08.794563Z","iopub.status.idle":"2025-08-13T16:55:08.832227Z","shell.execute_reply.started":"2025-08-13T16:55:08.794544Z","shell.execute_reply":"2025-08-13T16:55:08.831507Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def openImgaes(images, transform):\n    pil_images = []\n    for img_bytes in images:\n        if isinstance(img_bytes, bytes):\n            pil_img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n            pil_images.append(pil_img)\n        else:\n            pil_images.append(img_bytes)\n\n    # Apply provided transform\n    input_tensors = torch.stack([transform(img) for img in pil_images])\n    return input_tensors\n\n\ndef labels_to_tensor(labels):\n    label_to_idx = {\n      \"fake\": 0,\n       \"real\": 1\n    }\n    # labels is iterable of strings\n    numeric_labels = [label_to_idx[l] for l in labels]\n    return torch.tensor(numeric_labels, dtype=torch.long)\n\ndef evaluate_test_set(model, test_loader, class_names):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    acc = accuracy_score(all_labels, all_preds)\n    print(f\"Test set accuracy: {acc*100:.2f}%\")\n\ndef unfreeze_model(model, unfreeze_from_layer=6):\n    # ResNet layers: layer1, layer2, layer3, layer4\n    # unfreeze_from_layer: number between 1 and 4 to unfreeze from that layer onwards\n    layers = [model.layer1, model.layer2, model.layer3, model.layer4]\n\n    for param in model.parameters():\n        param.requires_grad = False  # Freeze all first\n\n    for param in model.fc.parameters():\n        param.requires_grad = True  # Always train final fc layer\n\n    # Unfreeze from specified layer onwards\n    for i in range(unfreeze_from_layer - 1, len(layers)):\n        for param in layers[i].parameters():\n            param.requires_grad = True\n\n    print(f\"Unfroze layers from layer{unfreeze_from_layer} onwards\")\n\ndef predict_batch(model, images, device, class_names):\n    model.eval()\n    images = images.to(device)\n    with torch.no_grad():\n        outputs = model(images)\n        probs = F.softmax(outputs, dim=1)\n        confidences, preds = torch.max(probs, 1)\n    pred_labels = [class_names[i] for i in preds.cpu().numpy()]\n    confs = confidences.cpu().numpy()\n    return pred_labels, confs\n\n\nfrom torch.utils.data import DataLoader\n\ndef get_label_from_key(key_str):\n\n    if 'fake' in key_str:\n        return 'fake'\n    elif 'real' in key_str:\n        return 'real'\n    else:\n        return 'unknown'\n\ndef preprocess(sample):\n    image = sample.get('png') or sample.get('jpg') or sample.get('tiff')\n    key = sample.get('__key__', '')\n    label = get_label_from_key(key)\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:55:08.833918Z","iopub.execute_input":"2025-08-13T16:55:08.834141Z","iopub.status.idle":"2025-08-13T16:55:08.850759Z","shell.execute_reply.started":"2025-08-13T16:55:08.834124Z","shell.execute_reply":"2025-08-13T16:55:08.850160Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Replace \"your_own_huggingface_token\" with your actual Hugging Face access token\n# Get one at: https://huggingface.co/settings/tokens\nmyTtoken = \"your_own_huggingface_token\"\n\nos.environ[\"HF_TOKEN\"] = myTtoken   \nsplits = {\n    \"train_fake\": \"**/fake_train/*.tar.gz\",\n    \"train_real\": \"**/real_train/*.tar.gz\",\n    \"test_fake\":  \"**/fake_test/*.tar.gz\",\n    \"test_real\":  \"**/real_test/*.tar.gz\"\n}\n\ndef get_urls(split_pattern):\n    fs = HfFileSystem()\n    files = [fs.resolve_path(path) for path in fs.glob(\"hf://datasets/xingjunm/WildDeepfake/\" + split_pattern)]\n    return [hf_hub_url(file.repo_id, file.path_in_repo, repo_type=\"dataset\") for file in files]\n\ndef make_ds(urls):\n    urls_pipe = f\"pipe: curl -s -L -H 'Authorization: Bearer {myTtoken}' {'::'.join(urls)}\"\n    return wds.WebDataset(urls_pipe, shardshuffle=False).decode()\n\n\ntrain_fake_urls = get_urls(splits[\"train_fake\"])\ntrain_real_urls = get_urls(splits[\"train_real\"])\ntest_fake_urls  = get_urls(splits[\"test_fake\"])\ntest_real_urls  = get_urls(splits[\"test_real\"])\n\nrandom.seed(42)  # For reproducibility\nrandom.shuffle(train_fake_urls)\nrandom.shuffle(train_real_urls)\nrandom.shuffle(test_fake_urls)\nrandom.shuffle(test_real_urls)\n\nprint(len(train_fake_urls), len(train_real_urls), len(test_fake_urls), len(test_real_urls), sep='\\n')\n\ntrain_fake_urls, val_fake_urls = train_fake_urls[:int(len(train_fake_urls)*0.8)], train_fake_urls[int(len(train_fake_urls)*0.8):]\ntrain_real_urls, val_real_urls = train_real_urls[:int(len(train_real_urls)*0.8)], train_real_urls[int(len(train_real_urls)*0.8):]\n\n\nmax_samples_training =2000\n\nmax_smaples_test= 500\n\nmax_smaples_val= 500\n\nfake_train = []\nmax_samples_per_url = 4\nfor url in train_fake_urls:\n  if len(fake_train) >= max_samples_training:\n    break\n  train_fake =make_ds([url])\n  train_fake_processed = list(train_fake.map(preprocess))\n  random.shuffle(train_fake_processed)\n  for image,label in train_fake_processed:\n    if len(fake_train) >= max_samples_training:\n      break\n    fake_train.append((image,label))\n\n# fake_train, fake_val = fake_data_set[:int(len(fake_data_set)*0.8)], fake_data_set[int(len(fake_data_set)*0.8):]\n# random.shuffle(fake_data_set)\nrandom.seed(100)\nrandom.shuffle(fake_train)\n\nreal_train = []\nmax_samples_per_url = 7\nfor url in train_real_urls:\n  if len(real_train) >= max_samples_training:\n    break\n  train_real =make_ds([url])\n  train_real_processed = list(train_real.map(preprocess))\n  random.shuffle(train_real_processed)\n  for image,label in train_real_processed:\n    if len(real_train) >= max_samples_training:\n      break\n    real_train.append((image,label))\n\n# real_train, real_val = real_data_set[:int(len(real_data_set)*0.8)], real_data_set[int(len(real_data_set)*0.8):]\nrandom.seed(5)\nrandom.shuffle(real_train)\n# random.seed(87)\n# random.shuffle(real_val)\n\ntest_fake_data_set = []\nmax_samples_per_url = 4\nfor url in test_fake_urls:\n  if len(test_fake_data_set) >= max_smaples_test:\n    break\n  test_fake =make_ds([url])\n  test_fake_processed = list(test_fake.map(preprocess))\n  random.shuffle(test_fake_processed)\n  for image,label in test_fake_processed:\n    if len(test_fake_data_set) >= max_smaples_test:\n      break\n    test_fake_data_set.append((image,label))\nrandom.seed(13)\nrandom.shuffle(test_fake_data_set)\n\ntest_real_data_set = []\nmax_samples_per_url = 13\nfor url in test_real_urls:\n  if len(test_real_data_set) >= max_smaples_test:\n    break\n  test_real =make_ds([url])\n  test_real_processed = list(test_real.map(preprocess))\n  random.shuffle(test_real_processed)\n  for image,label in test_real_processed:\n    if len(test_real_data_set) >= max_smaples_test:\n      break\n    test_real_data_set.append((image,label))\n\nrandom.seed(67)\nrandom.shuffle(test_real_data_set)\n\nfake_val = []\nmax_samples_per_url = 4\nfor url in val_fake_urls:\n  if len(fake_val) >= max_smaples_val:\n    break\n  val_fake =make_ds([url])\n  val_fake_processed = list(val_fake.map(preprocess))\n  random.shuffle(val_fake_processed)\n  for image,label in val_fake_processed:\n    if len(fake_val) >= max_smaples_val:\n      break\n    fake_val.append((image,label))\n\nrandom.seed(100)\nrandom.shuffle(fake_val)\n\nreal_val = []\nmax_samples_per_url = 7\nfor url in val_real_urls:\n  if len(real_val) >= max_smaples_val:\n    break\n  val_real =make_ds([url])\n  val_real_processed = list(val_real.map(preprocess))\n  random.shuffle(val_real_processed)\n  for image,label in val_real_processed:\n    if len(real_val) >= max_smaples_val:\n      break\n    real_val.append((image,label))\n\n\nrandom.seed(87)\nrandom.shuffle(real_val)\n\nprint(len(fake_train),len(real_train))\nprint(len(test_real_data_set),len(test_fake_data_set))\nprint(len(fake_val),len(real_val))\n\ntrain_set=fake_train + real_train\ntest_set= test_real_data_set + test_fake_data_set\nval_set = fake_val + real_val\n\nrandom.seed(58)\nrandom.shuffle(train_set)\nrandom.seed(1)\nrandom.shuffle(test_set)\nrandom.seed(99)\nrandom.shuffle(val_set)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:55:08.851567Z","iopub.execute_input":"2025-08-13T16:55:08.851813Z","iopub.status.idle":"2025-08-13T16:55:24.945744Z","shell.execute_reply.started":"2025-08-13T16:55:08.851773Z","shell.execute_reply":"2025-08-13T16:55:24.944975Z"}},"outputs":[{"name":"stdout","text":"592\n371\n115\n42\n2000 2000\n500 500\n500 500\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Load pretrained ConvNeXt Tiny and transforms\nweights = ConvNeXt_Tiny_Weights.IMAGENET1K_V1\ntrain_transform = weights.transforms()\nval_transform = weights.transforms()\n\nmodel = models.convnext_tiny(weights=weights)\n\n# Freeze all params initially\nfor param in model.parameters():\n    param.requires_grad = False\n\nnum_classes = 2  \nin_features = model.classifier[2].in_features\nmodel.classifier[2] = nn.Linear(in_features, num_classes)\n\nfor param in model.classifier.parameters():\n    param.requires_grad = True\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()\n\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:55:24.946697Z","iopub.execute_input":"2025-08-13T16:55:24.947004Z","iopub.status.idle":"2025-08-13T16:55:25.667032Z","shell.execute_reply.started":"2025-08-13T16:55:24.946977Z","shell.execute_reply":"2025-08-13T16:55:25.666256Z"}},"outputs":[{"name":"stdout","text":"ConvNeXt(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n    )\n    (1): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (1): Permute()\n          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=96, out_features=384, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=384, out_features=96, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (3): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (1): Permute()\n          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=192, out_features=768, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=768, out_features=192, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (5): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n      )\n      (3): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n      )\n      (4): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n      )\n      (5): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n      )\n      (6): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n      )\n      (7): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n      )\n      (8): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (1): Permute()\n          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=384, out_features=1536, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=1536, out_features=384, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (7): Sequential(\n      (0): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n      )\n      (1): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n      )\n      (2): CNBlock(\n        (block): Sequential(\n          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (1): Permute()\n          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (3): Linear(in_features=768, out_features=3072, bias=True)\n          (4): GELU(approximate='none')\n          (5): Linear(in_features=3072, out_features=768, bias=True)\n          (6): Permute()\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=768, out_features=2, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def mixup_data(x, y, alpha=0.4):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n\ndef train_progressive(model, train_dataset, val_dataset, device='cuda',\n                      epochs_fc=5, epochs_stage=5, epochs_all=5):\n\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32)\n\n    def run_epoch(train_loader, val_loader, use_mixup=False):\n        model.train()\n        total_loss = 0\n        for inputs, labels in train_loader:\n            inputs = openImgaes(inputs, train_transform).to(device)\n            labels_tensor = labels_to_tensor(labels).to(device)\n\n            optimizer.zero_grad()\n\n            if use_mixup:\n                inputs, y_a, y_b, lam = mixup_data(inputs, labels_tensor)\n                outputs = model(inputs)\n                loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n            else:\n                outputs = model(inputs)\n                loss = criterion(outputs, labels_tensor)\n\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n\n        # Validation\n        model.eval()\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs = openImgaes(inputs, val_transform).to(device)\n                labels_tensor = labels_to_tensor(labels).to(device)\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels_tensor.cpu().numpy())\n\n        acc = accuracy_score(all_labels, all_preds)\n        return avg_loss, acc\n        \n    optimizer = None\n    def phase_training(epochs, description, lr, use_mixup, unfreeze_stages):\n        nonlocal optimizer\n        # Unfreeze given stages (ConvNeXt has features[0..4] stages)\n        for stage_idx in unfreeze_stages:\n            for param in model.features[stage_idx].parameters():\n                param.requires_grad = True\n\n        print(f\"\\n[{description}]\")\n        best_acc = 0.0\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n        optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, model.parameters()),\n            lr=lr\n        )\n        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n\n        for epoch in range(epochs):\n            loss, acc = run_epoch(train_loader, val_loader, use_mixup=use_mixup)\n            print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}, Val Acc: {acc*100:.2f}%\")\n            scheduler.step(acc)\n\n            if acc > best_acc:\n                best_acc = acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save(best_model_wts, f\"best_model_{description.replace(' ', '_')}.pth\")\n\n        model.load_state_dict(best_model_wts)\n        print(f\"Restored best model for {description} with Val Acc: {best_acc*100:.2f}%\")\n\n    # Phase 1: only FC (classifier)\n    phase_training(epochs_fc, \"Phase 1 - FC only\", lr=1e-3, use_mixup=False, unfreeze_stages=[])\n\n    # Phase 2: unfreeze last ConvNeXt stage = features[4]\n    phase_training(epochs_stage, \"Phase 2 - FC + Last Stage\", lr=1e-4, use_mixup=True, unfreeze_stages=[4])\n\n    # Phase 3: unfreeze all remaining stages (features[0..3])\n    phase_training(epochs_all, \"Phase 3 - All layers\", lr=1e-5, use_mixup=True, unfreeze_stages=[0,1,2,3])\n\n    return model\n\n# Run training\nmodel = train_progressive(model, train_set, val_set, device=device,\n                          epochs_fc=15, epochs_stage=15, epochs_all=15)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T16:55:25.667809Z","iopub.execute_input":"2025-08-13T16:55:25.668062Z","iopub.status.idle":"2025-08-13T17:38:25.648319Z","shell.execute_reply.started":"2025-08-13T16:55:25.668045Z","shell.execute_reply":"2025-08-13T17:38:25.647582Z"}},"outputs":[{"name":"stdout","text":"\n[Phase 1 - FC only]\nEpoch 1/15 - Loss: 0.2178, Val Acc: 70.00%\nEpoch 2/15 - Loss: 0.0615, Val Acc: 70.40%\nEpoch 3/15 - Loss: 0.0350, Val Acc: 72.10%\nEpoch 4/15 - Loss: 0.0239, Val Acc: 73.20%\nEpoch 5/15 - Loss: 0.0172, Val Acc: 72.00%\nEpoch 6/15 - Loss: 0.0136, Val Acc: 73.30%\nEpoch 7/15 - Loss: 0.0107, Val Acc: 72.10%\nEpoch 8/15 - Loss: 0.0087, Val Acc: 73.50%\nEpoch 9/15 - Loss: 0.0075, Val Acc: 73.30%\nEpoch 10/15 - Loss: 0.0063, Val Acc: 74.10%\nEpoch 11/15 - Loss: 0.0059, Val Acc: 73.90%\nEpoch 12/15 - Loss: 0.0043, Val Acc: 73.60%\nEpoch 13/15 - Loss: 0.0039, Val Acc: 71.10%\nEpoch 14/15 - Loss: 0.0038, Val Acc: 74.70%\nEpoch 15/15 - Loss: 0.0033, Val Acc: 73.00%\nRestored best model for Phase 1 - FC only with Val Acc: 74.70%\n\n[Phase 2 - FC + Last Stage]\nEpoch 1/15 - Loss: 0.2668, Val Acc: 80.10%\nEpoch 2/15 - Loss: 0.2736, Val Acc: 80.20%\nEpoch 3/15 - Loss: 0.2355, Val Acc: 82.00%\nEpoch 4/15 - Loss: 0.2538, Val Acc: 87.20%\nEpoch 5/15 - Loss: 0.2125, Val Acc: 86.80%\nEpoch 6/15 - Loss: 0.1992, Val Acc: 89.30%\nEpoch 7/15 - Loss: 0.2222, Val Acc: 89.10%\nEpoch 8/15 - Loss: 0.2045, Val Acc: 87.80%\nEpoch 9/15 - Loss: 0.2355, Val Acc: 88.70%\nEpoch 10/15 - Loss: 0.2128, Val Acc: 89.00%\nEpoch 11/15 - Loss: 0.2082, Val Acc: 87.50%\nEpoch 12/15 - Loss: 0.2095, Val Acc: 89.40%\nEpoch 13/15 - Loss: 0.1830, Val Acc: 89.40%\nEpoch 14/15 - Loss: 0.1876, Val Acc: 88.20%\nEpoch 15/15 - Loss: 0.2059, Val Acc: 89.00%\nRestored best model for Phase 2 - FC + Last Stage with Val Acc: 89.40%\n\n[Phase 3 - All layers]\nEpoch 1/15 - Loss: 0.1995, Val Acc: 89.90%\nEpoch 2/15 - Loss: 0.2290, Val Acc: 90.00%\nEpoch 3/15 - Loss: 0.2216, Val Acc: 90.10%\nEpoch 4/15 - Loss: 0.2140, Val Acc: 90.30%\nEpoch 5/15 - Loss: 0.2098, Val Acc: 88.30%\nEpoch 6/15 - Loss: 0.2054, Val Acc: 92.20%\nEpoch 7/15 - Loss: 0.2217, Val Acc: 91.10%\nEpoch 8/15 - Loss: 0.2090, Val Acc: 91.80%\nEpoch 9/15 - Loss: 0.2047, Val Acc: 90.70%\nEpoch 10/15 - Loss: 0.2182, Val Acc: 88.60%\nEpoch 11/15 - Loss: 0.1860, Val Acc: 91.00%\nEpoch 12/15 - Loss: 0.1707, Val Acc: 89.10%\nEpoch 13/15 - Loss: 0.2086, Val Acc: 90.00%\nEpoch 14/15 - Loss: 0.1892, Val Acc: 91.20%\nEpoch 15/15 - Loss: 0.1924, Val Acc: 90.60%\nRestored best model for Phase 3 - All layers with Val Acc: 92.20%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model.eval()\nall_preds = []\nall_labels = []\nval_loader = DataLoader(test_set, batch_size=265)\nwith torch.no_grad():\n  for inputs, labels in val_loader:\n    newInputs = openImgaes(inputs,val_transform)\n    newInputs = newInputs.to(device)\n    labels_tensor = labels_to_tensor(labels).to(device)\n    outputs = model(newInputs)\n    _, preds = torch.max(outputs, 1)\n    all_preds.extend(preds.cpu().numpy())\n    all_labels.extend(labels_tensor.cpu().numpy())\n\nacc = accuracy_score(all_labels, all_preds)\nprint(f\"test Accuracy: {acc*100:.2f}%\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T18:01:29.929907Z","iopub.execute_input":"2025-08-13T18:01:29.930464Z","iopub.status.idle":"2025-08-13T18:01:35.624623Z","shell.execute_reply.started":"2025-08-13T18:01:29.930438Z","shell.execute_reply":"2025-08-13T18:01:35.623829Z"}},"outputs":[{"name":"stdout","text":"test Accuracy: 94.50%\n\n","output_type":"stream"}],"execution_count":9}]}